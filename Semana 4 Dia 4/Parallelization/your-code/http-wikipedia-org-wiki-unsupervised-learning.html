<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Unsupervised learning - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"38e72397-b924-4a52-a86f-5a5006b9fb29","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Unsupervised_learning","wgTitle":"Unsupervised learning","wgCurRevisionId":1016273690,"wgRevisionId":1016273690,"wgArticleId":233497,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: multiple names: authors list","Articles with short description","Short description matches Wikidata","Wikipedia articles with GND identifiers","Wikipedia articles with MA identifiers","Unsupervised learning","Machine learning"],"wgPageContentLanguage":"en",
"wgPageContentModel":"wikitext","wgRelevantPageName":"Unsupervised_learning","wgRelevantArticleId":233497,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsFlags":10,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q1152135"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.styles.legacy":
"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.36.0-wmf.38"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg/1200px-Multi-Layer_Neural_Network-Vector-Blank.svg.png"/>
<meta property="og:title" content="Unsupervised learning - Wikipedia"/>
<meta property="og:type" content="website"/>
<link rel="preconnect" href="//upload.wikimedia.org"/>
<link rel="alternate" media="only screen and (max-width: 720px)" href="//en.m.wikipedia.org/wiki/Unsupervised_learning"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Unsupervised_learning&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Unsupervised_learning&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Unsupervised_learning"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Unsupervised_learning rootpage-Unsupervised_learning skin-vector action-view skin-vector-legacy"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
	</div>
	<h1 id="firstHeading" class="firstHeading" >Unsupervised learning</h1>
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Machine learning technique</div>
<style data-mw-deduplicate="TemplateStyles:r1013635363">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar a{white-space:nowrap}.mw-parser-output .sidebar-wraplinks a{white-space:normal}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding-bottom:0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em 0}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding-top:0.2em;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding-top:0.4em;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.4em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding-top:0}.mw-parser-output .sidebar-image{padding:0.2em 0 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em}.mw-parser-output .sidebar-content{padding:0 0.1em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.4em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%}.mw-parser-output .sidebar-collapse .sidebar-navbar{padding-top:0.6em}.mw-parser-output .sidebar-list-title{text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><table class="sidebar sidebar-collapse nomobile"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br />and<br /><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-image"><a href="/wiki/File:Multi-Layer_Neural_Network-Vector-Blank.svg" class="image"><img alt="Multi-Layer Neural Network-Vector-Blank.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg/150px-Multi-Layer_Neural_Network-Vector-Blank.svg.png" decoding="async" width="150" height="72" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg/225px-Multi-Layer_Neural_Network-Vector-Blank.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg/300px-Multi-Layer_Neural_Network-Vector-Blank.svg.png 2x" data-file-width="815" data-file-height="390" /></a></td></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;">Problems</div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a class="mw-selflink selflink">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_data_clustering_algorithm" class="mw-redirect" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Cognitive_computing" title="Cognitive computing">Cognitive computing</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;">Theory</div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;">Machine-learning venues</div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="https://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="background:transparent;border-top:1px solid #aaa;text-align:center;">Related articles</div><div class="sidebar-list-content mw-collapsible-content"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td class="sidebar-navbar"><style data-mw-deduplicate="TemplateStyles:r992953826">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}.mw-parser-output .infobox .navbar{font-size:100%}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p><b>Unsupervised learning</b> (<b>UL</b>) is a type of algorithm that learns patterns from untagged data. The hope is that through mimicry, the machine is forced to build a compact internal representation of its world. In contrast to <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> (SL) where data is tagged by a human, e.g. as "car" or "fish" etc, UL exhibits self-organization that captures patterns as neuronal predilections or probability densities.<sup id="cite_ref-Hinton99a_1-0" class="reference"><a href="#cite_note-Hinton99a-1">&#91;1&#93;</a></sup> The other levels in the supervision spectrum are <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> where the machine is given only a numerical performance score as its guidance, and <a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-supervised learning</a> where a smaller portion of the data is tagged. Two broad methods in UL are Neural Networks and Probabilistic Methods.
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Probabilistic_methods"><span class="tocnumber">1</span> <span class="toctext">Probabilistic methods</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Approaches"><span class="tocnumber">1.1</span> <span class="toctext">Approaches</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Method_of_moments"><span class="tocnumber">1.2</span> <span class="toctext">Method of moments</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Neural_networks"><span class="tocnumber">2</span> <span class="toctext">Neural networks</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Basics"><span class="tocnumber">2.1</span> <span class="toctext">Basics</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Intermediate"><span class="tocnumber">2.2</span> <span class="toctext">Intermediate</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#See_also"><span class="tocnumber">3</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#References"><span class="tocnumber">4</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Further_reading"><span class="tocnumber">5</span> <span class="toctext">Further reading</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Probabilistic_methods">Probabilistic methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=1" title="Edit section: Probabilistic methods">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Two of the main methods used in unsupervised learning are <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component</a> and <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a>. <a href="/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a> is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships.<sup id="cite_ref-tds-ul_2-0" class="reference"><a href="#cite_note-tds-ul-2">&#91;2&#93;</a></sup> Cluster analysis is a branch of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> that groups the data that has not been <a href="/wiki/Labeled_data" title="Labeled data">labelled</a>, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.
</p><p>The only requirement to be called an unsupervised learning strategy is to learn a new feature space that captures the characteristics of the original space by maximizing some objective function or minimising some loss function. Therefore, generating a <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> is not unsupervised learning, but taking the <a href="/wiki/Eigenvectors" class="mw-redirect" title="Eigenvectors">eigenvectors</a> of the covariance matrix is because the linear algebra eigendecomposition operation maximizes the variance; this is known as principal component analysis.<sup id="cite_ref-vixra_3-0" class="reference"><a href="#cite_note-vixra-3">&#91;3&#93;</a></sup> Similarly, taking the log-transform of a dataset is not unsupervised learning, but passing input data through multiple sigmoid functions while minimising some distance function between the generated and resulting data is, and is known as an <a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a>.
</p><p>A central application of unsupervised learning is in the field of <a href="/wiki/Density_estimation" title="Density estimation">density estimation</a> in <a href="/wiki/Statistics" title="Statistics">statistics</a>,<sup id="cite_ref-JordanBishop2004_4-0" class="reference"><a href="#cite_note-JordanBishop2004-4">&#91;4&#93;</a></sup> though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It could be contrasted with supervised learning by saying that whereas supervised learning intends to infer a <a href="/wiki/Conditional_probability_distribution" title="Conditional probability distribution">conditional probability distribution</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle p_{X}(x\,|\,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>X</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mspace width="thinmathspace" />
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mspace width="thinmathspace" />
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle p_{X}(x\,|\,y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/475bcbc42afccf4fcd3770dfff0d137d9ffd980e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:8.607ex; height:2.843ex;" alt="{\textstyle p_{X}(x\,|\,y)}"/></span> conditioned on the label <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mi>y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db9936ddb2761b76fa640fb275cb5d1fa4d6fa23" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;" alt="{\textstyle y}"/></span> of input data; unsupervised learning intends to infer an <a href="/wiki/A_priori_probability" title="A priori probability">a priori probability</a> distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle p_{X}(x)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>X</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle p_{X}(x)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e7a0226309d31643df31b6abfcd9d1d6c56ec5ad" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:6.03ex; height:2.843ex;" alt="{\textstyle p_{X}(x)}"/></span>.
</p>
<h3><span class="mw-headline" id="Approaches">Approaches</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=2" title="Edit section: Approaches">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Neural Networks, and (4) Approaches for learning latent variable models.
Each approach uses several methods as follows:
</p>
<ul><li><a href="/wiki/Data_clustering" class="mw-redirect" title="Data clustering">Clustering</a> methods include: <a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">hierarchical clustering</a>,<sup id="cite_ref-Hastie_5-0" class="reference"><a href="#cite_note-Hastie-5">&#91;5&#93;</a></sup> <a href="/wiki/K-means" class="mw-redirect" title="K-means">k-means</a>,<sup id="cite_ref-tds-kmeans_6-0" class="reference"><a href="#cite_note-tds-kmeans-6">&#91;6&#93;</a></sup> <a href="/wiki/Mixture_models" class="mw-redirect" title="Mixture models">mixture models</a>, <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a>, and <a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS algorithm</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a> methods include: <a href="/wiki/Local_Outlier_Factor" class="mw-redirect" title="Local Outlier Factor">Local Outlier Factor</a>, and <a href="/wiki/Isolation_Forest" class="mw-redirect" title="Isolation Forest">Isolation Forest</a></li>
<li>Approaches for learning <a href="/wiki/Latent_variable_model" title="Latent variable model">latent variable models</a> such as <a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization algorithm</a> (EM), <a href="/wiki/Method_of_moments_(statistics)" title="Method of moments (statistics)">Method of moments</a>, and <a href="/wiki/Blind_signal_separation" class="mw-redirect" title="Blind signal separation">Blind signal separation</a> techniques (<a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a>, <a href="/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a>, <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Non-negative matrix factorization</a>, <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">Singular value decomposition</a> )</li></ul>
<h3><span class="mw-headline" id="Method_of_moments">Method of moments</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=3" title="Edit section: Method of moments">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>One of the statistical approaches for unsupervised learning is the <a href="/wiki/Method_of_moments_(statistics)" title="Method of moments (statistics)">method of moments</a>. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the <a href="/wiki/Mean" title="Mean">mean</a> vector, and the second order moment is the <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> (when the mean is zero). Higher order moments are usually represented using <a href="/wiki/Tensors" class="mw-redirect" title="Tensors">tensors</a> which are the generalization of matrices to higher orders as multi-dimensional arrays.
</p><p>In particular, the method of moments is shown to be effective in learning the parameters of <a href="/wiki/Latent_variable_model" title="Latent variable model">latent variable models</a>.<sup id="cite_ref-TensorLVMs_7-0" class="reference"><a href="#cite_note-TensorLVMs-7">&#91;7&#93;</a></sup>
Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the <a href="/wiki/Topic_modeling" class="mw-redirect" title="Topic modeling">topic modeling</a> which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.<sup id="cite_ref-TensorLVMs_7-1" class="reference"><a href="#cite_note-TensorLVMs-7">&#91;7&#93;</a></sup>
</p><p>The <a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization algorithm</a> (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.<sup id="cite_ref-TensorLVMs_7-2" class="reference"><a href="#cite_note-TensorLVMs-7">&#91;7&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Neural_networks">Neural networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=4" title="Edit section: Neural networks">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Basics">Basics</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=5" title="Edit section: Basics">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><b>First, some vocabulary:</b><br />
</p>
<table>

<tbody><tr style="vertical-align: top;">
<td><i>activation</i></td>
<td>= state value of the neuron. For binary neurons, this is usually 0 / 1, or +1 / -1.
</td></tr>
<tr style="vertical-align: top;">
<td><i>CAM</i></td>
<td>= content addressable memory. Recalling a memory by a partial pattern instead of a memory address.
</td></tr>
<tr style="vertical-align: top;">
<td><i>convergence</i></td>
<td>= the stabilization of an activation pattern on a network. In SL, convergence means stabilization of weights &amp; biases rather than activations.
</td></tr>
<tr style="vertical-align: top;">
<td><i>discriminative</i></td>
<td>= relating to recognition tasks. Also called analysis (in Pattern Theory), or inference.
</td></tr>
<tr style="vertical-align: top;">
<td><i>energy</i></td>
<td>= a macroscopic quantity describing the activation pattern in a network. (see below)
</td></tr>
<tr style="vertical-align: top;">
<td><i>generalization</i></td>
<td>= behaving accurately on previously un-encountered inputs
</td></tr>
<tr style="vertical-align: top;">
<td><i>generative</i></td>
<td>= Machine imagined and recall task. sometimes called synthesis (in Pattern Theory), mimicry, or deep fakes.
</td></tr>
<tr style="vertical-align: top;">
<td><i>inference</i></td>
<td>= the "run" phase (as opposed to training). During inference the network performs the task it is trained to do—either recognizing a pattern (SL) or creating one (UL). Usually inference descends the gradient of an energy function.  In contrast to SL, gradient descent occurs during training, NOT inference.
</td></tr>
<tr style="vertical-align: top;">
<td><i>machine vision</i></td>
<td>= machine learning on images.
</td></tr>
<tr style="vertical-align: top;">
<td><i>NLP</i></td>
<td>= Natural Language Processing. Machine learning of human languages.
</td></tr>
<tr style="vertical-align: top;">
<td><i>pattern</i></td>
<td>= network activations that has an internal order in some sense, or that can be described more compactly by features in the activations.  For example, the pixel pattern of a zero, whether it's given as data or imagined by the network, has a feature that is describable as a single loop. The features are encoded in the hidden neurons.
</td></tr>
<tr style="vertical-align: top;">
<td><i>training</i></td>
<td>= the learning phase. Here, the network adjusts its weights &amp; biases to learn from the inputs.
</td></tr></tbody></table>
<p><b>Tasks</b><br />
</p>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="/wiki/File:Task-guidance.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/9/90/Task-guidance.png/300px-Task-guidance.png" decoding="async" width="300" height="236" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/9/90/Task-guidance.png 1.5x" data-file-width="400" data-file-height="314" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Task-guidance.png" class="internal" title="Enlarge"></a></div>Tendency for a task to employ Supervised vs. Unsupervised methods</div></div></div>
<p>UL methods usually prepare a network for generative tasks rather than recognition, but grouping tasks as supervised or not can be hazy. For example, handwriting recognition started off in the 1980s as SL. Then in 2007, UL is used to prime the network for SL afterwards. Currently, SL has regained its position as the better method.
</p><p><b>Training</b><br />
During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (eg. its weights &amp; biases). This resembles the mimicry behavior of children as they learn a language. Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be express as an unstable high energy state in the network.
</p><p><b>Energy</b><br />
An energy function is a macroscopic measure of a network's state. This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion p <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \propto }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>&#x221D;<!-- ∝ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \propto }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0e3a55007ba2f092d6cafe6d33598e0608b81150" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.808ex; height:1.676ex;" alt="\propto "/></span> e<sup>E/kT</sup>, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is p = e<sup>-E</sup> / Z,<sup id="cite_ref-Hinton2010_8-0" class="reference"><a href="#cite_note-Hinton2010-8">&#91;8&#93;</a></sup> where p &amp; E vary over every possible activation pattern and Z = <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{AllPatterns}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>A</mi>
            <mi>l</mi>
            <mi>l</mi>
            <mi>P</mi>
            <mi>a</mi>
            <mi>t</mi>
            <mi>t</mi>
            <mi>e</mi>
            <mi>r</mi>
            <mi>n</mi>
            <mi>s</mi>
          </mrow>
        </munder>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{AllPatterns}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a1699c518b46dc076e28e6d8cda8bdc2b06e5f19" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.171ex; width:8.769ex; height:5.676ex;" alt="{\displaystyle \sum _{AllPatterns}}"/></span> e <sup>-E(pattern)</sup>. To be more precise, p(a) = e<sup>-E(a)</sup> / Z, where a is an activation pattern of all neurons (visible and hidden). Hence, early neural networks bear the name Boltzmann Machine. Paul Smolensky calls -E the Harmony. A network seeks low energy which is high Harmony.
</p><p><b>Networks</b><br />
</p>
<table class="wikitable">

<tbody><tr>
<th>Hopfield</th>
<th>Boltzmann</th>
<th>RBM</th>
<th>Helmholtz</th>
<th>Autoencoder</th>
<th>VAE
</th></tr>
<tr>
<td><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Hopfield-net-vector.svg" class="image"><img alt="Hopfield-net-vector.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/44/Hopfield-net-vector.svg/220px-Hopfield-net-vector.svg.png" decoding="async" width="220" height="252" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/44/Hopfield-net-vector.svg/330px-Hopfield-net-vector.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/44/Hopfield-net-vector.svg/440px-Hopfield-net-vector.svg.png 2x" data-file-width="730" data-file-height="835" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Hopfield-net-vector.svg" class="internal" title="Enlarge"></a></div></div></div></div></td>
<td><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Boltzmannexamplev1.png" class="image"><img alt="Boltzmannexamplev1.png" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Boltzmannexamplev1.png/220px-Boltzmannexamplev1.png" decoding="async" width="220" height="209" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Boltzmannexamplev1.png/330px-Boltzmannexamplev1.png 1.5x, //upload.wikimedia.org/wikipedia/commons/7/7a/Boltzmannexamplev1.png 2x" data-file-width="433" data-file-height="411" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Boltzmannexamplev1.png" class="internal" title="Enlarge"></a></div></div></div></div></td>
<td><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Restricted_Boltzmann_machine.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Restricted_Boltzmann_machine.svg/220px-Restricted_Boltzmann_machine.svg.png" decoding="async" width="220" height="234" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Restricted_Boltzmann_machine.svg/330px-Restricted_Boltzmann_machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Restricted_Boltzmann_machine.svg/440px-Restricted_Boltzmann_machine.svg.png 2x" data-file-width="310" data-file-height="330" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Restricted_Boltzmann_machine.svg" class="internal" title="Enlarge"></a></div>restricted Boltzmann machine</div></div></div></td>
<td><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Helmholtz_Machine.png" class="image"><img alt="Helmholtz Machine.png" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e2/Helmholtz_Machine.png/220px-Helmholtz_Machine.png" decoding="async" width="220" height="261" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/e/e2/Helmholtz_Machine.png 1.5x" data-file-width="273" data-file-height="324" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Helmholtz_Machine.png" class="internal" title="Enlarge"></a></div></div></div></div>
</td>
<td><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Autoencoder_schema.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/220px-Autoencoder_schema.png" decoding="async" width="220" height="200" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/330px-Autoencoder_schema.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/440px-Autoencoder_schema.png 2x" data-file-width="841" data-file-height="765" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Autoencoder_schema.png" class="internal" title="Enlarge"></a></div>autoencoder</div></div></div></td>
<td><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:VAE_blocks.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/5/5e/VAE_blocks.png/220px-VAE_blocks.png" decoding="async" width="220" height="200" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/5e/VAE_blocks.png/330px-VAE_blocks.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/5e/VAE_blocks.png/440px-VAE_blocks.png 2x" data-file-width="841" data-file-height="765" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:VAE_blocks.png" class="internal" title="Enlarge"></a></div>variational autoencoder</div></div></div>
</td></tr></tbody></table>
<p>Boltzmann and Helmholtz came before neural networks formulations, but these networks borrowed from their analyses, so these networks bear their names. Hopfield, however, directly contributed to UL.
</p>
<h3><span class="mw-headline" id="Intermediate">Intermediate</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=6" title="Edit section: Intermediate">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Here, distributions p(x) and q(x) will be abbreviated as p and q.
</p><p><b>History</b><br />
</p>
<table border="0">

<tbody><tr style="vertical-align: top;">
<td>1969</td>
<td>Perceptrons by Minsky &amp; Papert shows a perceptron without hidden layers fails on XOR
</td></tr>
<tr style="vertical-align: top;">
<td>1970s</td>
<td>(approximate dates) AI winter I
</td></tr>
<tr style="vertical-align: top;">
<td>1974</td>
<td>Ising magnetic model proposed by WA Little for cognition
</td></tr>
<tr style="vertical-align: top;">
<td>1980</td>
<td>Fukushima introduces the neocognitron, which is later called a convolution neural network. It is mostly used in SL, but deserves a mention here.
</td></tr>
<tr style="vertical-align: top;">
<td>1982</td>
<td>Ising variant Hopfield net described as CAMs and classifiers by John Hopfield.
</td></tr>
<tr style="vertical-align: top;">
<td>1983</td>
<td>Ising variant Boltzmann machine with probabilistic neurons described by Hinton &amp; Sejnowski following Sherington &amp; Kirkpatrick's 1975 work.
</td></tr>
<tr style="vertical-align: top;">
<td>1986</td>
<td>Paul Smolensky publishes Harmony Theory, which is an RBM with practically the same Boltzmann energy function. Smolensky did not give an practical training scheme. Hinton did in mid-2000s
</td></tr>
<tr style="vertical-align: top;">
<td>1995</td>
<td>Schmidthuber introduces the LSTM neuron for languages.
</td></tr>
<tr style="vertical-align: top;">
<td>1995</td>
<td>Dayan &amp; Hinton introduces Helmholtz machine
</td></tr>
<tr style="vertical-align: top;">
<td>1995-2005</td>
<td>(approximate dates) AI winter II
</td></tr>
<tr style="vertical-align: top;">
<td>2013</td>
<td>Kingma, Rezende, &amp; co. introduced Variational Autoencoders as Bayesian graphical probability network, with neural nets as components.
</td></tr></tbody></table>
<p><b>Some more vocabulary:</b><br />
</p>
<table>

<tbody><tr style="vertical-align: top;">
<td><b>Probability</b></td>
<td>
</td></tr>
<tr style="vertical-align: top;">
<td><i>cdf</i></td>
<td>= cumulative distribution function. the integral of the pdf. The probability of getting near 3 is the area under the curve between 2.9 and 3.1.
</td></tr>
<tr style="vertical-align: top;">
<td><i>contrastive divergence</i></td>
<td>= a learning method where one lowers the energy on training patterns and raises the energy on unwanted patterns outside of the training set. This is very different from the KL-divergence, but shares a similar wording.
</td></tr>
<tr style="vertical-align: top;">
<td><i>expected value </i></td>
<td>= E(x) = <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{x}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </munder>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{x}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/656fda37c270cf5a92f338adf6f6b40ed22bbb1c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:3.355ex; height:5.509ex;" alt="{\displaystyle \sum _{x}}"/></span> x * p(x). This is the mean value, or average value. For continuous input x, replace the summation with an integral.
</td></tr>
<tr style="vertical-align: top;">
<td><i>latent variable</i></td>
<td>= an unobserved quantity that helps to explain observed data. for instance, a flu infection (unobserved) can explain why the a person sneezes (observed). In probabilistic neural networks, hidden neurons act as latent variables, though their latent interpretation is not explicitly known.
</td></tr>
<tr style="vertical-align: top;">
<td><i>pdf</i></td>
<td>= probability density function. The probability that a random variable takes on a certain value. For continuous pdf, p(3) = 1/2 can still mean there is near zero chance of achieving this exact value of 3. We rationalize this with the cdf.
</td></tr>
<tr style="vertical-align: top;">
<td><i>stochastic</i></td>
<td>= behaves according to a well described probability density formula.
</td></tr>
<tr>
<td><b>Thermodynamics</b></td>
<td>
</td></tr>
<tr style="vertical-align: top;">
<td><i>Boltzmann distribution</i></td>
<td>= Gibbs distribution. p <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \propto }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>&#x221D;<!-- ∝ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \propto }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0e3a55007ba2f092d6cafe6d33598e0608b81150" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.808ex; height:1.676ex;" alt="\propto "/></span> e<sup>E/kT</sup>
</td></tr>
<tr style="vertical-align: top;">
<td><i>entropy</i></td>
<td>= expected information = <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{x}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </munder>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{x}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/656fda37c270cf5a92f338adf6f6b40ed22bbb1c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:3.355ex; height:5.509ex;" alt="{\displaystyle \sum _{x}}"/></span> p * log p
</td></tr>
<tr style="vertical-align: top;">
<td><i>Gibbs free energy</i></td>
<td>= thermodynamic potential. It's the maximum reversible work that may be performed by a heat system at constant temperature and pressure. free energy G = heat - temperature * entropy
</td></tr>
<tr style="vertical-align: top;">
<td><i>information</i></td>
<td>= the information amount of a message x = -log p(x)
</td></tr>
<tr style="vertical-align: top;">
<td><i>KLD</i></td>
<td>= relative entropy. For probabilistic networks, this is the analogue of the error between input &amp; mimicked output. The Kullback-Liebler divergence (KLD) measures the entropy deviation of 1 distribution from another distribution. KLD(p,q) = <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{x}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </munder>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{x}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/656fda37c270cf5a92f338adf6f6b40ed22bbb1c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:3.355ex; height:5.509ex;" alt="{\displaystyle \sum _{x}}"/></span> p * log( p / q ). Typically, p reflects the input data, q reflects the network's interpretation of it, and KLD reflects the difference between the two.
</td></tr></tbody></table>
<p><b>Comparison of Networks</b>
</p>
<table class="wikitable">

<tbody><tr>
<th></th>
<th>Hopfield</th>
<th>Boltzmann</th>
<th>RBM</th>
<th>Helmholtz</th>
<th>Autoencoder</th>
<th>VAE
</th></tr>
<tr>
<td><b>usage &amp; notables</b></td>
<td>CAM, traveling salesman problem</td>
<td>CAM. The freedom of connections makes this network difficult to analyze.</td>
<td>pattern recognition (MNIST, speech recognition)</td>
<td>imagination, mimicry</td>
<td>language: creative writing, translation. Vision: enhancing blurry images</td>
<td>generate realistic data
</td></tr>
<tr>
<td><b>neuron</b></td>
<td>deterministic binary state. Activation = { 0 (or -1) if x is negative, 1 otherwise }</td>
<td>stochastic binary Hopfield neuron</td>
<td>stochastic binary. Extended to real-valued in mid 2000s</td>
<td>binary, sigmoid</td>
<td>language: LSTM. vision: local receptive fields. usually real valued relu activation.</td>
<td>
</td></tr>
<tr>
<td><b>connections</b></td>
<td>1-layer with symmetric weights. No self-connections.</td>
<td>2-layers. 1-hidden &amp; 1-visible. symmetric weights.</td>
<td>2-layers. symmetric weights. no lateral connections within a layer.</td>
<td>3-layers: asymmetric weights. 2 networks combined into 1.</td>
<td>3-layers. The input is considered a layer even though it has no inbound weights. recurrent layers for NLP. feedforward convolutions for vision. input &amp; output have the same neuron counts.</td>
<td>3-layers: input, encoder, distribution sampler decoder. the sampler is not considered a layer (e)
</td></tr>
<tr>
<td><b>inference &amp; energy</b></td>
<td>energy is given by Gibbs probability measure&#160;:<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E=-{\frac {1}{2}}\sum _{i,j}{w_{ij}{s_{i}}{s_{j}}}+\sum _{i}{\theta _{i}}{s_{i}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo>=</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mn>2</mn>
          </mfrac>
        </mrow>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi>w</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
              <mi>j</mi>
            </mrow>
          </msub>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>s</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>s</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </mrow>
        <mo>+</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi>&#x03B8;<!-- θ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi>s</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E=-{\frac {1}{2}}\sum _{i,j}{w_{ij}{s_{i}}{s_{j}}}+\sum _{i}{\theta _{i}}{s_{i}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76ad57f7823ca1b74ab67b5f3dadbc83d24da7c2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:30.204ex; height:6.676ex;" alt="E=-{\frac  12}\sum _{{i,j}}{w_{{ij}}{s_{i}}{s_{j}}}+\sum _{i}{\theta _{i}}{s_{i}}"/></span></td>
<td>← same</td>
<td>← same</td>
<td>minimize KL divergence</td>
<td>inference is only feed-forward. previous UL networks ran forwards AND backwards</td>
<td>minimize error = reconstruction error - KLD
</td></tr>
<tr>
<td><b>training</b></td>
<td>Δw<sub>ij</sub> = s<sub>i</sub>*s<sub>j</sub>, for +1/-1 neuron</td>
<td>Δw<sub>ij</sub> = e*(p<sub>ij</sub> - p'<sub>ij</sub>). This is derived from minimizing KLD. e = learning rate, p' = predicted and p = actual distribution.
</td>
<td>contrastive divergence w/ Gibbs Sampling</td>
<td>wake-sleep 2 phase training</td>
<td>back propagate the reconstruction error</td>
<td>reparameterize hidden state for backprop
</td></tr>
<tr>
<td><b>strength</b></td>
<td>resembles physical systems so it inherits their equations</td>
<td>&lt;--- same. hidden neurons act as internal representatation of the external world</td>
<td>faster more practical training scheme than Boltzmann machines</td>
<td>mildly anatomical. analyzable w/ information theory &amp; statistical mechanics</td>
<td></td>
<td>
</td></tr>
<tr>
<td><b>weakness</b></td>
<td>hopfield</td>
<td>hard to train due to lateral connections</td>
<td>RBM</td>
<td>Helmholtz</td>
<td></td>
<td>
</td></tr></tbody></table>
<p><b>Specific Networks</b><br />
Here, we highlight some characteristics of each networks. Ferromagnetism inspired Hopfield networks, Boltzmann machines, and RBMs. A neuron correspond to an iron domain with binary magnetic moments Up and Down, and neural connections correspond to the domain's influence on each other. Symmetric connections enables a global energy formulation. During inference the network updates each state using the standard activation step function. Symmetric weights guarantees convergence to a stable activation pattern.<br />
<b>Hopfield</b> networks are used as CAMs and are guaranteed to settle to a some pattern. Without symmetric weights, the network is very hard to analyze. With the right energy function, a network will converge.<br />
<b>Boltzmann machines</b> are stochastic Hopfield nets. Their state value is sampled from this pdf as follows: suppose a binary neuron fires with the Bernoulli probability p(1) = 1/3 and rests with p(0) = 2/3. One samples from it by taking a UNIFORMLY distributed random number y, and plugging it into the inverted cumulative distribution function, which in this case is the step function thresholded at 2/3. The inverse function = { 0 if x &lt;= 2/3, 1 if x &gt; 2/3 }<br />
<b>Helmholtz</b> machines are early inspirations for the Variational Auto Encoders. It's 2 networks combined into one—forward weights operates recognition and backward weights implements imagination. It is perhaps the first network to do both. Helmholtz did not work in machine learning but he inspired the view of "statistical inference engine whose function is to infer probable causes of sensory input" (3). the stochastic binary neuron outputs a probability that its state is 0 or 1. The data input is normally not considered a layer, but in the Helmholtz machine generation mode, the data layer receives input from the middle layer has separate weights for this purpose, so it is considered a layer. Hence this network has 3 layers.<br />
<b>Variational Autoencoder</b> (VAE) are inspired by Helmholtz machines and combines probability network with neural networks. An Autoencoder is a 3-layer CAM network, where the middle layer is supposed to be some internal representation of input patterns. The weights are named phi &amp; theta rather than W and V as in Helmholtz—a cosmetic difference. The encoder neural network is a probability distribution q<sub>φ</sub>(z|x) and the decoder network is p<sub>θ</sub>(x|z). These 2 networks here can be fully connected, or use another NN scheme.
</p><p><b>Hebbian Learning, ART, SOM</b><br />
The classical example of unsupervised learning in the study of neural networks is <a href="/wiki/Donald_Hebb" class="mw-redirect" title="Donald Hebb">Donald Hebb</a>'s principle, that is, neurons that fire together wire together.<sup id="cite_ref-Buhmann_9-0" class="reference"><a href="#cite_note-Buhmann-9">&#91;9&#93;</a></sup> In <a href="/wiki/Hebbian_learning" class="mw-redirect" title="Hebbian learning">Hebbian learning</a>, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons.<sup id="cite_ref-Comesana_10-0" class="reference"><a href="#cite_note-Comesana-10">&#91;10&#93;</a></sup> A similar version that modifies synaptic weights takes into account the time between the action potentials (<a href="/wiki/Spike-timing-dependent_plasticity" title="Spike-timing-dependent plasticity">spike-timing-dependent plasticity</a> or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and experiential learning.
</p><p>Among <a href="/wiki/Artificial_neural_network" title="Artificial neural network">neural network</a> models, the <a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a> (SOM) and <a href="/wiki/Adaptive_resonance_theory" title="Adaptive resonance theory">adaptive resonance theory</a> (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as <a href="/wiki/Automatic_target_recognition" title="Automatic target recognition">automatic target recognition</a> and seismic signal processing.<sup id="cite_ref-Carpenter_11-0" class="reference"><a href="#cite_note-Carpenter-11">&#91;11&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=7" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">Automated machine learning</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization algorithm</a></li>
<li><a href="/wiki/Generative_topographic_map" title="Generative topographic map">Generative topographic map</a></li>
<li><a href="/wiki/Meta-learning_(computer_science)" class="mw-redirect" title="Meta-learning (computer science)">Meta-learning (computer science)</a></li>
<li><a href="/wiki/Multivariate_analysis" title="Multivariate analysis">Multivariate analysis</a></li>
<li><a href="/wiki/Radial_basis_function_network" title="Radial basis function network">Radial basis function network</a></li>
<li><a href="/wiki/Weak_supervision" title="Weak supervision">Weak supervision</a></li></ul>
<p><br />
</p>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=8" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-Hinton99a-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-Hinton99a_1-0">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r999302996">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite id="CITEREFHintonSejnowski1999" class="citation book cs1">Hinton, Geoffrey; Sejnowski, Terrence (1999). <i>Unsupervised Learning: Foundations of Neural Computation</i>. MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0262581684" title="Special:BookSources/978-0262581684"><bdi>978-0262581684</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Unsupervised+Learning%3A+Foundations+of+Neural+Computation&amp;rft.pub=MIT+Press&amp;rft.date=1999&amp;rft.isbn=978-0262581684&amp;rft.aulast=Hinton&amp;rft.aufirst=Geoffrey&amp;rft.au=Sejnowski%2C+Terrence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-tds-ul-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-tds-ul_2-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFRoman2019" class="citation web cs1">Roman, Victor (2019-04-21). <a rel="nofollow" class="external text" href="https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e">"Unsupervised Machine Learning: Clustering Analysis"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-10-01</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Unsupervised+Machine+Learning%3A+Clustering+Analysis&amp;rft.date=2019-04-21&amp;rft.aulast=Roman&amp;rft.aufirst=Victor&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-machine-learning-clustering-analysis-d40f2b34ae7e&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-vixra-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-vixra_3-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFSnow2020" class="citation journal cs1">Snow, Dr Derek (2020-03-26). <a rel="nofollow" class="external text" href="https://vixra.org/abs/2003.0581">"Machine Learning in Asset Management: Part 2: Portfolio Construction—Weight Optimization"</a>. <i>Journal of Financial Data Science</i>. <b>2</b> (2): 17–24. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3905%2Fjfds.2020.1.029">10.3905/jfds.2020.1.029</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:215932953">215932953</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2020-05-16</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Financial+Data+Science&amp;rft.atitle=Machine+Learning+in+Asset+Management%3A+Part+2%3A+Portfolio+Construction%E2%80%94Weight+Optimization&amp;rft.volume=2&amp;rft.issue=2&amp;rft.pages=17-24&amp;rft.date=2020-03-26&amp;rft_id=info%3Adoi%2F10.3905%2Fjfds.2020.1.029&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A215932953%23id-name%3DS2CID&amp;rft.aulast=Snow&amp;rft.aufirst=Dr+Derek&amp;rft_id=https%3A%2F%2Fvixra.org%2Fabs%2F2003.0581&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-JordanBishop2004-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-JordanBishop2004_4-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFJordanBishop2004" class="citation book cs1">Jordan, Michael I.; Bishop, Christopher M. (2004). "Neural Networks".  In Allen B. Tucker (ed.). <i>Computer Science Handbook, Second Edition (Section VII: Intelligent Systems)</i>. Boca Raton, Florida: Chapman &amp; Hall/CRC Press LLC. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1-58488-360-X" title="Special:BookSources/1-58488-360-X"><bdi>1-58488-360-X</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Neural+Networks&amp;rft.btitle=Computer+Science+Handbook%2C+Second+Edition+%28Section+VII%3A+Intelligent+Systems%29&amp;rft.place=Boca+Raton%2C+Florida&amp;rft.pub=Chapman+%26+Hall%2FCRC+Press+LLC&amp;rft.date=2004&amp;rft.isbn=1-58488-360-X&amp;rft.aulast=Jordan&amp;rft.aufirst=Michael+I.&amp;rft.au=Bishop%2C+Christopher+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-Hastie-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-Hastie_5-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFHastie,_Trevor,_Robert_Tibshirani2009" class="citation book cs1">Hastie, Trevor, Robert Tibshirani, Friedman, Jerome (2009). <i>The Elements of Statistical Learning: Data mining, Inference, and Prediction</i>. New York: Springer. pp.&#160;485–586. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-387-84857-0" title="Special:BookSources/978-0-387-84857-0"><bdi>978-0-387-84857-0</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Elements+of+Statistical+Learning%3A+Data+mining%2C+Inference%2C+and+Prediction&amp;rft.place=New+York&amp;rft.pages=485-586&amp;rft.pub=Springer&amp;rft.date=2009&amp;rft.isbn=978-0-387-84857-0&amp;rft.aulast=Hastie%2C+Trevor%2C+Robert+Tibshirani&amp;rft.aufirst=Friedman%2C+Jerome&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: multiple names: authors list (<a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">link</a>)</span></span>
</li>
<li id="cite_note-tds-kmeans-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-tds-kmeans_6-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFGarbade2018" class="citation web cs1">Garbade, Dr Michael J. (2018-09-12). <a rel="nofollow" class="external text" href="https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1">"Understanding K-means Clustering in Machine Learning"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-10-31</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Understanding+K-means+Clustering+in+Machine+Learning&amp;rft.date=2018-09-12&amp;rft.aulast=Garbade&amp;rft.aufirst=Dr+Michael+J.&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-means-clustering-in-machine-learning-6a6e67336aa1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-TensorLVMs-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-TensorLVMs_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-TensorLVMs_7-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-TensorLVMs_7-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFAnandkumarGeHsuKakade2014" class="citation journal cs1">Anandkumar, Animashree; Ge, Rong; Hsu, Daniel; Kakade, Sham; Telgarsky, Matus (2014). <a rel="nofollow" class="external text" href="http://www.jmlr.org/papers/volume15/anandkumar14b/anandkumar14b.pdf">"Tensor Decompositions for Learning Latent Variable Models"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>15</b>: 2773–2832. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1210.7559">1210.7559</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2012arXiv1210.7559A">2012arXiv1210.7559A</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Tensor+Decompositions+for+Learning+Latent+Variable+Models&amp;rft.volume=15&amp;rft.pages=2773-2832&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1210.7559&amp;rft_id=info%3Abibcode%2F2012arXiv1210.7559A&amp;rft.aulast=Anandkumar&amp;rft.aufirst=Animashree&amp;rft.au=Ge%2C+Rong&amp;rft.au=Hsu%2C+Daniel&amp;rft.au=Kakade%2C+Sham&amp;rft.au=Telgarsky%2C+Matus&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume15%2Fanandkumar14b%2Fanandkumar14b.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-Hinton2010-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-Hinton2010_8-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFHinton,_G2010" class="citation news cs1">Hinton, G (2010-08-02). "A Practical Guide to Training Restricted Boltzmann Machines".</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A+Practical+Guide+to+Training+Restricted+Boltzmann+Machines&amp;rft.date=2010-08-02&amp;rft.au=Hinton%2C+G&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-Buhmann-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-Buhmann_9-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFBuhmannKuhnel1992" class="citation book cs1">Buhmann, J.; Kuhnel, H. (1992). "Unsupervised and supervised data clustering with competitive neural networks". <i>&#91;Proceedings 1992&#93; IJCNN International Joint Conference on Neural Networks</i>. <b>4</b>. IEEE. pp.&#160;796–801. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2Fijcnn.1992.227220">10.1109/ijcnn.1992.227220</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0780305590" title="Special:BookSources/0780305590"><bdi>0780305590</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:62651220">62651220</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Unsupervised+and+supervised+data+clustering+with+competitive+neural+networks&amp;rft.btitle=%26%2391%3BProceedings+1992%26%2393%3B+IJCNN+International+Joint+Conference+on+Neural+Networks&amp;rft.pages=796-801&amp;rft.pub=IEEE&amp;rft.date=1992&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A62651220%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2Fijcnn.1992.227220&amp;rft.isbn=0780305590&amp;rft.aulast=Buhmann&amp;rft.aufirst=J.&amp;rft.au=Kuhnel%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-Comesana-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-Comesana_10-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFComesaña-CamposBouza-Rodríguez2016" class="citation journal cs1">Comesaña-Campos, Alberto; Bouza-Rodríguez, José Benito (June 2016). <a rel="nofollow" class="external text" href="https://www.semanticscholar.org/paper/4059b77be03fea077350c106e6e9aa9fce23e8c7">"An application of Hebbian learning in the design process decision-making"</a>. <i>Journal of Intelligent Manufacturing</i>. <b>27</b> (3): 487–506. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10845-014-0881-z">10.1007/s10845-014-0881-z</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0956-5515">0956-5515</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:207171436">207171436</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Intelligent+Manufacturing&amp;rft.atitle=An+application+of+Hebbian+learning+in+the+design+process+decision-making&amp;rft.volume=27&amp;rft.issue=3&amp;rft.pages=487-506&amp;rft.date=2016-06&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A207171436%23id-name%3DS2CID&amp;rft.issn=0956-5515&amp;rft_id=info%3Adoi%2F10.1007%2Fs10845-014-0881-z&amp;rft.aulast=Comesa%C3%B1a-Campos&amp;rft.aufirst=Alberto&amp;rft.au=Bouza-Rodr%C3%ADguez%2C+Jos%C3%A9+Benito&amp;rft_id=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2F4059b77be03fea077350c106e6e9aa9fce23e8c7&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-Carpenter-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-Carpenter_11-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFCarpenter,_G.A.Grossberg,_S.1988" class="citation journal cs1">Carpenter, G.A. &amp; Grossberg, S. (1988). <a rel="nofollow" class="external text" href="http://www.cns.bu.edu/Profiles/Grossberg/CarGro1988Computer.pdf">"The ART of adaptive pattern recognition by a self-organizing neural network"</a> <span class="cs1-format">(PDF)</span>. <i>Computer</i>. <b>21</b> (3): 77–88. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2F2.33">10.1109/2.33</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:14625094">14625094</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computer&amp;rft.atitle=The+ART+of+adaptive+pattern+recognition+by+a+self-organizing+neural+network&amp;rft.volume=21&amp;rft.issue=3&amp;rft.pages=77-88&amp;rft.date=1988&amp;rft_id=info%3Adoi%2F10.1109%2F2.33&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A14625094%23id-name%3DS2CID&amp;rft.au=Carpenter%2C+G.A.&amp;rft.au=Grossberg%2C+S.&amp;rft_id=http%3A%2F%2Fwww.cns.bu.edu%2FProfiles%2FGrossberg%2FCarGro1988Computer.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit&amp;section=9" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFBousquet,_O.von_Luxburg,_U.Raetsch,_G.2004" class="citation book cs1">Bousquet, O.; von Luxburg, U.; Raetsch, G., eds. (2004). <a rel="nofollow" class="external text" href="https://archive.org/details/springer_10.1007-b100712"><i>Advanced Lectures on Machine Learning</i></a>. Springer-Verlag. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3540231226" title="Special:BookSources/978-3540231226"><bdi>978-3540231226</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Advanced+Lectures+on+Machine+Learning&amp;rft.pub=Springer-Verlag&amp;rft.date=2004&amp;rft.isbn=978-3540231226&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fspringer_10.1007-b100712&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFDuda,_Richard_O.Hart,_Peter_E.Stork,_David_G.2001" class="citation book cs1"><a href="/wiki/Richard_O._Duda" title="Richard O. Duda">Duda, Richard O.</a>; <a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart, Peter E.</a>; Stork, David G. (2001). "Unsupervised Learning and Clustering". <a href="/wiki/Pattern_classification" class="mw-redirect" title="Pattern classification"><i>Pattern classification</i></a> (2nd&#160;ed.). Wiley. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-471-05669-3" title="Special:BookSources/0-471-05669-3"><bdi>0-471-05669-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Unsupervised+Learning+and+Clustering&amp;rft.btitle=Pattern+classification&amp;rft.edition=2nd&amp;rft.pub=Wiley&amp;rft.date=2001&amp;rft.isbn=0-471-05669-3&amp;rft.au=Duda%2C+Richard+O.&amp;rft.au=Hart%2C+Peter+E.&amp;rft.au=Stork%2C+David+G.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFHastieTibshirani2009" class="citation book cs1">Hastie, Trevor; Tibshirani, Robert (2009). <i>The Elements of Statistical Learning: Data mining, Inference, and Prediction</i>. New York: Springer. pp.&#160;485–586. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-0-387-84858-7_14">10.1007/978-0-387-84858-7_14</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-387-84857-0" title="Special:BookSources/978-0-387-84857-0"><bdi>978-0-387-84857-0</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Elements+of+Statistical+Learning%3A+Data+mining%2C+Inference%2C+and+Prediction&amp;rft.place=New+York&amp;rft.pages=485-586&amp;rft.pub=Springer&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1007%2F978-0-387-84858-7_14&amp;rft.isbn=978-0-387-84857-0&amp;rft.aulast=Hastie&amp;rft.aufirst=Trevor&amp;rft.au=Tibshirani%2C+Robert&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r999302996"/><cite id="CITEREFHinton,_GeoffreySejnowski,_Terrence_J.1999" class="citation book cs1"><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Hinton, Geoffrey</a>; <a href="/wiki/Terrence_J._Sejnowski" class="mw-redirect" title="Terrence J. Sejnowski">Sejnowski, Terrence J.</a>, eds. (1999). <i>Unsupervised Learning: Foundations of Neural Computation</i>. <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-262-58168-X" title="Special:BookSources/0-262-58168-X"><bdi>0-262-58168-X</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Unsupervised+Learning%3A+Foundations+of+Neural+Computation&amp;rft.pub=MIT+Press&amp;rft.date=1999&amp;rft.isbn=0-262-58168-X&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AUnsupervised+learning" class="Z3988"></span> (This book focuses on unsupervised learning in <a href="/wiki/Neural_network" title="Neural network">neural networks</a>)</li></ul>
<div role="navigation" class="navbox" aria-labelledby="Differentiable_computing" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="3"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r992953826"/><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Differentiable_computing" title="Template:Differentiable computing"><abbr title="View this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Differentiable_computing" title="Template talk:Differentiable computing"><abbr title="Discuss this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Differentiable_computing&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">e</abbr></a></li></ul></div><div id="Differentiable_computing" style="font-size:114%;margin:0 4em">Differentiable computing</div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Differentiable_function" title="Differentiable function">General</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Differentiable_programming" title="Differentiable programming">Differentiable programming</a></li>
<li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Automatic_differentiation" title="Automatic differentiation">Automatic differentiation</a></li>
<li><a href="/wiki/Neuromorphic_engineering" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Cable_theory" title="Cable theory">Cable theory</a></li>
<li><a href="/wiki/Pattern_recognition" title="Pattern recognition">Pattern recognition</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Tensor_calculus" title="Tensor calculus">Tensor calculus</a></li></ul>
</div></td><td class="noviewer navbox-image" rowspan="9" style="width:1px;padding:0px 0px 0px 2px"><div><div class="floatright"><a href="/wiki/File:DNC_training_recall_task.gif" class="image"><img alt="DNC training recall task.gif" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/200px-DNC_training_recall_task.gif" decoding="async" width="200" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/300px-DNC_training_recall_task.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b5/DNC_training_recall_task.gif/400px-DNC_training_recall_task.gif 2x" data-file-width="919" data-file-height="459" /></a></div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/wiki/Batch_normalization" title="Batch normalization">Normalization</a></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_sets" title="Training, validation, and test sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Programming languages</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a></li>
<li><a href="/wiki/Julia_(programming_language)" title="Julia (programming language)">Julia</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Application</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Computational_science" title="Computational science">Scientific computing</a></li>
<li><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial Intelligence</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Hardware</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Graphcore" title="Graphcore">IPU</a></li>
<li><a href="/wiki/Tensor_processing_unit" class="mw-redirect" title="Tensor processing unit">TPU</a></li>
<li><a href="/wiki/Vision_processing_unit" title="Vision processing unit">VPU</a></li>
<li><a href="/wiki/Memristor" title="Memristor">Memristor</a></li>
<li><a href="/wiki/SpiNNaker" title="SpiNNaker">SpiNNaker</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Software library</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a></li>
<li><a href="/wiki/PyTorch" title="PyTorch">PyTorch</a></li>
<li><a href="/wiki/Keras" title="Keras">Keras</a></li>
<li><a href="/wiki/Theano_(software)" title="Theano (software)">Theano</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementation</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio-visual</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Speech_synthesis" title="Speech synthesis">Speech synthesis</a></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Verbal</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/Watson_(computer)" title="Watson (computer)">Watson</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">GPT-2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">GPT-3</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li>
<li><a href="/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory" title="MIT Computer Science and Artificial Intelligence Laboratory">MIT CSAIL</a></li>
<li><a href="/wiki/Mila_(research_institute)" title="Mila (research institute)">Mila</a></li>
<li><a href="/wiki/Google_Brain" title="Google Brain">Google Brain</a></li>
<li><a href="https://fr.wikipedia.org/wiki/Facebook_Artificial_Intelligence_Research" class="extiw" title="fr:Facebook Artificial Intelligence Research">FAIR</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="3"><div>
<ul><li><img alt="Portal" src="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/16px-Symbol_portal_class.svg.png" decoding="async" title="Portal" width="16" height="16" srcset="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/23px-Symbol_portal_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/31px-Symbol_portal_class.svg.png 2x" data-file-width="180" data-file-height="185" /> Portals
<ul><li><a href="/wiki/Portal:Computer_programming" title="Portal:Computer programming">Computer programming</a></li>
<li><a href="/wiki/Portal:Technology" title="Portal:Technology">Technology</a></li></ul></li>
<li><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png" decoding="async" title="Category" width="16" height="16" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x" data-file-width="180" data-file-height="185" /> Category
<ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li>
<li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></li></ul>
</div></td></tr></tbody></table></div>
<div role="navigation" class="navbox authority-control" aria-labelledby="Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q1152135#identifiers&amp;#124;Edit_this_at_Wikidata" style="padding:3px"><table class="nowraplinks hlist navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th id="Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q1152135#identifiers&amp;#124;Edit_this_at_Wikidata" scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Help:Authority_control" title="Help:Authority control">Authority control</a> <a href="https://www.wikidata.org/wiki/Q1152135#identifiers" title="Edit this at Wikidata"><img alt="Edit this at Wikidata" src="//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png" decoding="async" width="10" height="10" style="vertical-align: text-top" srcset="//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x" data-file-width="20" data-file-height="20" /></a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><span class="nowrap"><a href="/wiki/GND_(identifier)" class="mw-redirect" title="GND (identifier)">GND</a>: <span class="uid"><a rel="nofollow" class="external text" href="https://d-nb.info/gnd/4580265-8">4580265-8</a></span></span></li>
<li><span class="nowrap"><a href="/wiki/MA_(identifier)" class="mw-redirect" title="MA (identifier)">MA</a>: <span class="uid"><a rel="nofollow" class="external text" href="https://academic.microsoft.com/v2/detail/8038995">8038995</a></span></span></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1350
Cached time: 20210414013523
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.591 seconds
Real time usage: 0.944 seconds
Preprocessor visited node count: 1314/1000000
Post‐expand include size: 82826/2097152 bytes
Template argument size: 1331/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 51624/5000000 bytes
Lua time usage: 0.302/10.000 seconds
Lua memory usage: 5921525/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  737.548      1 -total
 47.46%  350.020      1 Template:Reflist
 32.43%  239.162      8 Template:Cite_book
 18.85%  138.995      1 Template:Machine_learning_bar
 17.50%  129.046      1 Template:Sidebar_with_collapsible_lists
 12.27%   90.492      1 Template:Short_description
  9.35%   68.963      1 Template:Authority_control
  6.98%   51.514      1 Template:Differentiable_computing
  6.79%   50.069      1 Template:Pagetype
  6.52%   48.103      2 Template:Navbox
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:233497-0!canonical!math=5 and timestamp 20210414013522 and revision id 1016273690. Serialized with JSON.
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Unsupervised_learning&amp;oldid=1016273690">https://en.wikipedia.org/w/index.php?title=Unsupervised_learning&amp;oldid=1016273690</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_multiple_names:_authors_list" title="Category:CS1 maint: multiple names: authors list">CS1 maint: multiple names: authors list</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:Wikipedia_articles_with_GND_identifiers" title="Category:Wikipedia articles with GND identifiers">Wikipedia articles with GND identifiers</a></li><li><a href="/wiki/Category:Wikipedia_articles_with_MA_identifiers" title="Category:Wikipedia articles with MA identifiers">Wikipedia articles with MA identifiers</a></li></ul></div></div>
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-menu" aria-labelledby="p-personal-label" role="navigation" 
	 >
	<h3 id="p-personal-label" class="vector-menu-heading">
		<span>Personal tools</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Unsupervised+learning" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Unsupervised+learning" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li></ul>
		
	</div>
</nav>

		<div id="left-navigation">
			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" aria-labelledby="p-namespaces-label" role="navigation" 
	 >
	<h3 id="p-namespaces-label" class="vector-menu-heading">
		<span>Namespaces</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li id="ca-nstab-main" class="selected"><a href="/wiki/Unsupervised_learning" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Unsupervised_learning" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t">Talk</a></li></ul>
		
	</div>
</nav>

			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu vector-menu-dropdown" aria-labelledby="p-variants-label" role="navigation" 
	 >
	<input type="checkbox" class="vector-menu-checkbox" aria-labelledby="p-variants-label" />
	<h3 id="p-variants-label" class="vector-menu-heading">
		<span>Variants</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" aria-labelledby="p-views-label" role="navigation" 
	 >
	<h3 id="p-views-label" class="vector-menu-heading">
		<span>Views</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li id="ca-view" class="selected"><a href="/wiki/Unsupervised_learning">Read</a></li><li id="ca-edit"><a href="/w/index.php?title=Unsupervised_learning&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history"><a href="/w/index.php?title=Unsupervised_learning&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li></ul>
		
	</div>
</nav>

			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu vector-menu-dropdown" aria-labelledby="p-cactions-label" role="navigation" 
	 >
	<input type="checkbox" class="vector-menu-checkbox" aria-labelledby="p-cactions-label" />
	<h3 id="p-cactions-label" class="vector-menu-heading">
		<span>More</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

			<div id="p-search" role="search" >
	<h3 >
		<label for="searchInput">Search</label>
	</h3>
	<form action="/w/index.php" id="searchform">
		<div id="simpleSearch" data-search-loc="header-navigation">
			<input type="search" name="search" placeholder="Search Wikipedia" autocapitalize="sentences" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
			<input type="hidden" name="title" value="Special:Search"/>
			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
		</div>
	</form>
</div>

		</div>
	</div>
	
<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Main_Page"
			title="Visit the main page"></a>
	</div>
	<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal" aria-labelledby="p-navigation-label" role="navigation" 
	 >
	<h3 id="p-navigation-label" class="vector-menu-heading">
		<span>Navigation</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Articles related to current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li></ul>
		
	</div>
</nav>

	<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-interaction" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" aria-labelledby="p-interaction-label" role="navigation" 
	 >
	<h3 id="p-interaction-label" class="vector-menu-heading">
		<span>Contribute</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-introduction"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia">Learn to edit</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>
		
	</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" aria-labelledby="p-tb-label" role="navigation" 
	 >
	<h3 id="p-tb-label" class="vector-menu-heading">
		<span>Tools</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Unsupervised_learning" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Unsupervised_learning" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Unsupervised_learning&amp;oldid=1016273690" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Unsupervised_learning&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Unsupervised_learning&amp;id=1016273690&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q1152135" title="Structured data on this page hosted by Wikidata [g]" accesskey="g">Wikidata item</a></li></ul>
		
	</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-coll-print_export" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" aria-labelledby="p-coll-print_export-label" role="navigation" 
	 >
	<h3 id="p-coll-print_export-label" class="vector-menu-heading">
		<span>Print/export</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Unsupervised_learning&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Unsupervised_learning&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
		
	</div>
</nav>

	<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-lang" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" aria-labelledby="p-lang-label" role="navigation" 
	 >
	<h3 id="p-lang-label" class="vector-menu-heading">
		<span>Languages</span>
	</h3>
	<div class="vector-menu-content">
		<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D8%AA%D8%B9%D9%84%D9%85_%D8%BA%D9%8A%D8%B1_%D9%85%D8%B1%D8%A7%D9%82%D8%A8" title="تعلم غير مراقب – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-zh-min-nan"><a href="https://zh-min-nan.wikipedia.org/wiki/B%C3%B4-k%C3%A0m-tok_ha%CC%8Dk-si%CC%8Dp" title="Bô-kàm-tok ha̍k-si̍p – Chinese (Min Nan)" lang="nan" hreflang="nan" class="interlanguage-link-target">Bân-lâm-gú</a></li><li class="interlanguage-link interwiki-cs"><a href="https://cs.wikipedia.org/wiki/U%C4%8Den%C3%AD_bez_u%C4%8Ditele" title="Učení bez učitele – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target">Čeština</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Un%C3%BCberwachtes_Lernen" title="Unüberwachtes Lernen – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-el"><a href="https://el.wikipedia.org/wiki/%CE%9C%CE%B7-%CE%B5%CF%80%CE%B9%CE%B2%CE%BB%CE%B5%CF%80%CF%8C%CE%BC%CE%B5%CE%BD%CE%B7_%CE%9C%CE%AC%CE%B8%CE%B7%CF%83%CE%B7" title="Μη-επιβλεπόμενη Μάθηση – Greek" lang="el" hreflang="el" class="interlanguage-link-target">Ελληνικά</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Aprendizaje_no_supervisado" title="Aprendizaje no supervisado – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D8%A8%DB%8C%E2%80%8C%D9%86%D8%B8%D8%A7%D8%B1%D8%AA" title="یادگیری بی‌نظارت – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Apprentissage_non_supervis%C3%A9" title="Apprentissage non supervisé – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-ko"><a href="https://ko.wikipedia.org/wiki/%EB%B9%84%EC%A7%80%EB%8F%84_%ED%95%99%EC%8A%B5" title="비지도 학습 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target">한국어</a></li><li class="interlanguage-link interwiki-id"><a href="https://id.wikipedia.org/wiki/Pemelajaran_tak_terarah" title="Pemelajaran tak terarah – Indonesian" lang="id" hreflang="id" class="interlanguage-link-target">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Apprendimento_non_supervisionato" title="Apprendimento non supervisionato – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-he"><a href="https://he.wikipedia.org/wiki/%D7%9C%D7%9E%D7%99%D7%93%D7%94_%D7%91%D7%9C%D7%AA%D7%99_%D7%9E%D7%95%D7%A0%D7%97%D7%99%D7%AA" title="למידה בלתי מונחית – Hebrew" lang="he" hreflang="he" class="interlanguage-link-target">עברית</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%AD%A6%E7%BF%92" title="教師なし学習 – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">日本語</a></li><li class="interlanguage-link interwiki-pl"><a href="https://pl.wikipedia.org/wiki/Uczenie_nienadzorowane" title="Uczenie nienadzorowane – Polish" lang="pl" hreflang="pl" class="interlanguage-link-target">Polski</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B1%D0%B5%D0%B7_%D1%83%D1%87%D0%B8%D1%82%D0%B5%D0%BB%D1%8F" title="Обучение без учителя – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-ckb"><a href="https://ckb.wikipedia.org/wiki/%D9%81%DB%8E%D8%B1%D8%A8%D9%88%D9%88%D9%86%DB%8C_%DA%86%D8%A7%D9%88%D8%AF%DB%8E%D8%B1%DB%8C%D9%86%DB%95%DA%A9%D8%B1%D8%A7%D9%88" title="فێربوونی چاودێرینەکراو – Central Kurdish" lang="ckb" hreflang="ckb" class="interlanguage-link-target">کوردی</a></li><li class="interlanguage-link interwiki-fi"><a href="https://fi.wikipedia.org/wiki/Ohjaamaton_oppiminen" title="Ohjaamaton oppiminen – Finnish" lang="fi" hreflang="fi" class="interlanguage-link-target">Suomi</a></li><li class="interlanguage-link interwiki-tl"><a href="https://tl.wikipedia.org/wiki/Hindi_pinapatnubayang_pagkatuto" title="Hindi pinapatnubayang pagkatuto – Tagalog" lang="tl" hreflang="tl" class="interlanguage-link-target">Tagalog</a></li><li class="interlanguage-link interwiki-th"><a href="https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%80%E0%B8%A3%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%A1%E0%B8%B5%E0%B8%9C%E0%B8%B9%E0%B9%89%E0%B8%AA%E0%B8%AD%E0%B8%99" title="การเรียนรู้แบบไม่มีผู้สอน – Thai" lang="th" hreflang="th" class="interlanguage-link-target">ไทย</a></li><li class="interlanguage-link interwiki-tr"><a href="https://tr.wikipedia.org/wiki/G%C3%B6zetimsiz_%C3%B6%C4%9Frenme" title="Gözetimsiz öğrenme – Turkish" lang="tr" hreflang="tr" class="interlanguage-link-target">Türkçe</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F_%D0%B1%D0%B5%D0%B7_%D1%83%D1%87%D0%B8%D1%82%D0%B5%D0%BB%D1%8F" title="Навчання без учителя – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-vi"><a href="https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_kh%C3%B4ng_c%C3%B3_gi%C3%A1m_s%C3%A1t" title="Học không có giám sát – Vietnamese" lang="vi" hreflang="vi" class="interlanguage-link-target">Tiếng Việt</a></li><li class="interlanguage-link interwiki-zh-yue"><a href="https://zh-yue.wikipedia.org/wiki/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92" title="非監督式學習 – Cantonese" lang="yue" hreflang="yue" class="interlanguage-link-target">粵語</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E7%84%A1%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92" title="無監督學習 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q1152135#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
	</div>
</nav>

</div>

</div>
<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info" >
	<li id="footer-info-lastmod"> This page was last edited on 6 April 2021, at 09:08<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places" >
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Unsupervised_learning&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"/></a></li>
</ul>

</footer>


<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.591","walltime":"0.944","ppvisitednodes":{"value":1314,"limit":1000000},"postexpandincludesize":{"value":82826,"limit":2097152},"templateargumentsize":{"value":1331,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":51624,"limit":5000000},"entityaccesscount":{"value":1,"limit":400},"timingprofile":["100.00%  737.548      1 -total"," 47.46%  350.020      1 Template:Reflist"," 32.43%  239.162      8 Template:Cite_book"," 18.85%  138.995      1 Template:Machine_learning_bar"," 17.50%  129.046      1 Template:Sidebar_with_collapsible_lists"," 12.27%   90.492      1 Template:Short_description","  9.35%   68.963      1 Template:Authority_control","  6.98%   51.514      1 Template:Differentiable_computing","  6.79%   50.069      1 Template:Pagetype","  6.52%   48.103      2 Template:Navbox"]},"scribunto":{"limitreport-timeusage":{"value":"0.302","limit":"10.000"},"limitreport-memusage":{"value":5921525,"limit":52428800}},"cachereport":{"origin":"mw1350","timestamp":"20210414013523","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Unsupervised learning","url":"https:\/\/en.wikipedia.org\/wiki\/Unsupervised_learning","sameAs":"http:\/\/www.wikidata.org\/entity\/Q1152135","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q1152135","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2003-05-25T06:42:27Z","dateModified":"2021-04-06T09:08:35Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/00\/Multi-Layer_Neural_Network-Vector-Blank.svg","headline":"machine learning technique"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":177,"wgHostname":"mw1328"});});</script>
</body></html>